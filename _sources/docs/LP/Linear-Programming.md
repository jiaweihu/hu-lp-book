# Linear Programming 

We've discussed in an earlier video that there are different optimization procedures that you can use to solve optimization problems. And the optimization procedure that you'll pick depends on the nature of your problem. In this video, we'll understand how linear programming works. Linear programming is a method of achieving the best outcome in a mathematical model whose requirements are represented by linear relationships. So, essentially an optimization procedure that allows you to solve optimization problems where the requirements are linear in nature. Let's understand the characteristics or the nature of the problems where we would choose to use a linear programming. You should check to see whether all of your decision variables are continuous and can take on any value within the range. In order to be able to use linear programming as your optimization procedure, your decision variables should be real or continuous.
Linear programming places constraints on the objective function as well the objective function must be a linear function. It should be expressed in linear terms. This means that the objective function should not have polynomial terms such as X square, X cube and so on. And finally the left-hand side of all of the constraint equations must also be linear functions, and these can be either equalities or inequalities. You can see here that in order to use linear programming to solve your optimization model, a number of requirements have to be met by our decision variables, objective function as well as the constraints. Here is a general format of how you might express your objective function. The objective function is usually represented using Z,

[Video description begins] A slide titled Objective Function appears. It displays the following equation: Z= c1x1 + c2x2 + c3x3 ... + cnxn. [Video description ends]

and you can see here that the objective function is linear. Every x term that represents a decision variable is raised to the power of 1. So, there are no polynomial terms here in this objective function. This objective function will represent something that you need to either maximize or minimize.
All of the constraints also need to be specified as linear equalities or inequalities.

[Video description begins] Another slide named Constraints appears. It contains the following lines of equation: First line reads: a11x1 + a12x2 + a13x3 . . . + a1nxn ≤ b1. Second line reads: a21x1 + a22x2 + a23x3 . . . + a2nxn ≤ b2. Third line reads: .... Fourth line reads: .... Fifth line reads: .... Sixth line reads: am1x1 + am2x2 + am3x3 . . .+ amnxn ≤ bm. [Video description ends]

Once again, all of the x terms are the decision variables that are part of your constraint equations. Here, all of the constraints happen to be less than equal to inequalities. But they can be equalities as well or even greater than equal to inequalities. And the objective function put together along with the constraints will give you the standard form of the linear programming problem. We have the objective function, one that we'll either maximize or minimize, expressed in a linear form. And we have all of the constraints expressed in linear form as well.
Now, let's take a look at some of the assumptions of linear programming. The first is the assumption of proportionality. Now, this assumption states that the contribution of individual variables in the objective functions and constraints are proportional. Since there are no polynomial terms, it means that a particular decision variable doesn't contribute more or less within a particular function. There is also the assumption of additivity. The total value of the objective function and each constraint function is obtained by adding up the individual contributions. There is a coefficient for every decision variable. But that single component is added up to other components. So, the individual components are not multiplied together or divided. Then, we have the assumption of divisibility. The decision variables are allowed to take on any real numeric values within the range specified by the constraints. So, the decision variables can be subdivided very finely and they'll make sense. If you remember this is a characteristic feature of continuous data. And finally, we have the assumption of certainty. The parameter values in the model are known with certainty or at least treated as though they are known.
Let's dive into each of these assumptions in more detail, starting with the proportionality assumption. Now, we assume that setting up production for your widget or anything else that you're making includes no startup cost. If they were startup costs, that would be a constant term in the equation. Now, this assumption of startup costs will violate proportionality. That is why we assume startup costs are equal to 0. If startup costs existed, the more you produce, the greater the profit per unit of production. By assuming no startup costs, we know that the profit per unit of production will remain the same and that's how we model our problem. And on the flip side, if they were indeed startup costs involved, the lesser the production, smaller the profit per unit of production. So, the startup cost is immortalized over fewer units giving you less profit per unit. Now, because we assume that the profit per unit remains the same, we assume no startup cost. This is very important for the proportionality assumption.
The additivity assumption basically means that we assume that the units that we produce. If you're producing multiple products, they are not complementary. Complementary goods tend to have a positive synergy. So as you produce more of one good, more of the second good is needed. If you produce more printers, you need more cartridges. These are complementary goods. With complementary goods, as your scale increases, your cost reduce or profits increase. So, with linear programming the assumption is that the goods that you're producing are not complementary. The additivity assumption also means that your goods are not substitutes. Now, substitute goods have negative synergies as scales increases, costs will increase or profits will reduce. If a company makes two different models of printers, the sales of one printer might eat into the sales of the second printer. Those are examples of substitute goods.
And finally, a linear programming makes the divisibility assumption. Fractional values of decision variables, half a batch or three-quarters of a batch or three-quarters of a product are acceptable in the optimal solution. Now, if you feel that your units of production cannot be subdivided, you will not use linear programming instead you'll use a different optimization technique that is known as integer programming.

